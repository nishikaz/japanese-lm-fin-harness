diff --git a/developments/make_harness_sh.py b/developments/make_harness_sh.py
index 694a303..4ed19e3 100644
--- a/developments/make_harness_sh.py
+++ b/developments/make_harness_sh.py
@@ -133,13 +133,15 @@ def main() -> None:
         )
         values = ",".join(
             [
-                prefix + ":" + str(values["metric_values"])
+                prefix
+                + ":"
+                + str(values["metric_values"] * (1.5 if prefix == "chabsa" else 1.0))
                 for prefix, values in best_metrics.items()
             ]
         )
         harness_command = f"""MODEL_ARGS="{','.join(model_args)}"
 TASK="{task}"
-python main.py --model {'hf-causal' if cast(str, model_setting['model']).startswith('pfnet/plamo') else 'hf-causal-experimental'} --model_args $MODEL_ARGS --tasks $TASK --num_fewshot "{num_fewshots}" --output_path "models/{model_name}/result.json"
+python main.py --model {'hf-causal' if cast(str, model_setting['model']).startswith('pfnet/plamo') else 'hf-causal-experimental'} --model_args $MODEL_ARGS --tasks $TASK --num_fewshot "{num_fewshots}" --no_cache --output_path "models/{model_name}/result.json"
 # Estimated results: {values}
 """
         save_path = os.path.join(args.outputs_path, model_name, "harness.sh")
diff --git a/main.py b/main.py
index 4d43328..b2dbf64 100644
--- a/main.py
+++ b/main.py
@@ -11,6 +11,8 @@ from lm_eval.models.gpt3 import GPT3LM
 from lm_eval.models.gpt3 import get_result
 from lm_eval.models.gpt3 import oa_completion
 from tqdm import tqdm
+from transformers.models.auto.tokenization_auto import TOKENIZER_MAPPING_NAMES
+from transformers.models.auto.tokenization_auto import AutoTokenizer
 
 from jlm_fin_eval import evaluator
 from jlm_fin_eval import tasks
@@ -20,6 +22,16 @@ openai.api_base = os.environ.get("OPENAI_API_BASE", "https://api.openai.com/v1")
 openai.api_version = os.environ.get("OPENAI_API_VERSION")
 openai.api_key = os.environ.get("OPENAI_API_SECRET_KEY")
 
+# QWenTokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)
+# transformers.models.QWenTokenizer = QWenTokenizer
+# TOKENIZER_MAPPING_NAMES["QWenTokenizer"] = (None, "QWenTokenizer")
+
+
+def from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs):
+    return AutoTokenizer._from_pretrained(
+        pretrained_model_name_or_path, *inputs, trust_remote_code=True, **kwargs
+    )
+
 
 class MultiChoice:
     def __init__(self, choices: List[str]) -> None:
@@ -127,6 +139,10 @@ def main() -> None:
 
     assert not args.provide_description  # not implemented
 
+    if "trust_remote_code=True" in args.model_args:
+        AutoTokenizer._from_pretrained = AutoTokenizer.from_pretrained
+        AutoTokenizer.from_pretrained = from_pretrained
+
     if args.limit:
         print(
             "WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT."
