diff --git a/developments/make_harness_sh.py b/developments/make_harness_sh.py
index 694a303..4ed19e3 100644
--- a/developments/make_harness_sh.py
+++ b/developments/make_harness_sh.py
@@ -133,13 +133,15 @@ def main() -> None:
         )
         values = ",".join(
             [
-                prefix + ":" + str(values["metric_values"])
+                prefix
+                + ":"
+                + str(values["metric_values"] * (1.5 if prefix == "chabsa" else 1.0))
                 for prefix, values in best_metrics.items()
             ]
         )
         harness_command = f"""MODEL_ARGS="{','.join(model_args)}"
 TASK="{task}"
-python main.py --model {'hf-causal' if cast(str, model_setting['model']).startswith('pfnet/plamo') else 'hf-causal-experimental'} --model_args $MODEL_ARGS --tasks $TASK --num_fewshot "{num_fewshots}" --output_path "models/{model_name}/result.json"
+python main.py --model {'hf-causal' if cast(str, model_setting['model']).startswith('pfnet/plamo') else 'hf-causal-experimental'} --model_args $MODEL_ARGS --tasks $TASK --num_fewshot "{num_fewshots}" --no_cache --output_path "models/{model_name}/result.json"
 # Estimated results: {values}
 """
         save_path = os.path.join(args.outputs_path, model_name, "harness.sh")
diff --git a/developments/models/rinna.json b/developments/models/rinna.json
index 2a7a53b..35400cd 100644
--- a/developments/models/rinna.json
+++ b/developments/models/rinna.json
@@ -78,5 +78,45 @@
         "n_gpu": 1,
         "gpu_vram_gb": 24,
         "require_hf_login": false
+    },
+    {
+        "model": "rinna/nekomata-14b",
+        "model_args": [
+            "trust_remote_code=True"
+        ],
+        "memory_Gi": 128,
+        "n_gpu": 1,
+        "gpu_vram_gb": 80,
+        "require_hf_login": false
+    },
+    {
+        "model": "rinna/nekomata-14b-instruction",
+        "model_args": [
+            "trust_remote_code=True"
+        ],
+        "memory_Gi": 128,
+        "n_gpu": 1,
+        "gpu_vram_gb": 80,
+        "require_hf_login": false
+    },
+    {
+        "model": "rinna/nekomata-7b",
+        "model_args": [
+            "trust_remote_code=True"
+        ],
+        "memory_Gi": 48,
+        "n_gpu": 1,
+        "gpu_vram_gb": 24,
+        "require_hf_login": false
+    },
+    {
+        "model": "rinna/nekomata-7b-instruction",
+        "model_args": [
+            "trust_remote_code=True"
+        ],
+        "memory_Gi": 48,
+        "n_gpu": 1,
+        "gpu_vram_gb": 24,
+        "require_hf_login": false
     }
 ]
\ No newline at end of file
diff --git a/developments/models/stabilityai.json b/developments/models/stabilityai.json
index b46d35e..f0aace8 100644
--- a/developments/models/stabilityai.json
+++ b/developments/models/stabilityai.json
@@ -31,5 +31,29 @@
         "n_gpu": 1,
         "gpu_vram_gb": 32,
         "require_hf_login": false
+    },
+    {
+        "model": "stabilityai/japanese-stablelm-base-beta-70b",
+        "model_args": [
+            "trust_remote_code=True",
+            "use_accelerate=True",
+            "device_map_option=auto"
+        ],
+        "memory_Gi": 256,
+        "n_gpu": 8,
+        "gpu_vram_gb": 32,
+        "require_hf_login": false
+    },
+    {
+        "model": "stabilityai/japanese-stablelm-instruct-beta-70b",
+        "model_args": [
+            "trust_remote_code=True",
+            "use_accelerate=True",
+            "device_map_option=auto"
+        ],
+        "memory_Gi": 256,
+        "n_gpu": 8,
+        "gpu_vram_gb": 32,
+        "require_hf_login": false
     }
 ]
\ No newline at end of file
diff --git a/developments/models/swallow.json b/developments/models/swallow.json
index bc9f0e1..7adfec8 100644
--- a/developments/models/swallow.json
+++ b/developments/models/swallow.json
@@ -34,23 +34,23 @@
     {
         "model": "tokyotech-llm/Swallow-70b-hf",
         "model_args": [
-            "use_accelerate=false",
+            "use_accelerate=true",
             "device_map_option=auto"
         ],
         "memory_Gi": 256,
-        "n_gpu": 4,
-        "gpu_vram_gb": 80,
+        "n_gpu": 8,
+        "gpu_vram_gb": 32,
         "require_hf_login": false
     },
     {
         "model": "tokyotech-llm/Swallow-70b-instruct-hf",
         "model_args": [
-            "use_accelerate=false",
+            "use_accelerate=true",
             "device_map_option=auto"
         ],
         "memory_Gi": 256,
-        "n_gpu": 4,
-        "gpu_vram_gb": 80,
+        "n_gpu": 8,
+        "gpu_vram_gb": 32,
         "require_hf_login": false
     }
 ]
\ No newline at end of file
diff --git a/main.py b/main.py
index 4d43328..b2dbf64 100644
--- a/main.py
+++ b/main.py
@@ -11,6 +11,8 @@ from lm_eval.models.gpt3 import GPT3LM
 from lm_eval.models.gpt3 import get_result
 from lm_eval.models.gpt3 import oa_completion
 from tqdm import tqdm
+from transformers.models.auto.tokenization_auto import TOKENIZER_MAPPING_NAMES
+from transformers.models.auto.tokenization_auto import AutoTokenizer
 
 from jlm_fin_eval import evaluator
 from jlm_fin_eval import tasks
@@ -20,6 +22,16 @@ openai.api_base = os.environ.get("OPENAI_API_BASE", "https://api.openai.com/v1")
 openai.api_version = os.environ.get("OPENAI_API_VERSION")
 openai.api_key = os.environ.get("OPENAI_API_SECRET_KEY")
 
+# QWenTokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)
+# transformers.models.QWenTokenizer = QWenTokenizer
+# TOKENIZER_MAPPING_NAMES["QWenTokenizer"] = (None, "QWenTokenizer")
+
+
+def from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs):
+    return AutoTokenizer._from_pretrained(
+        pretrained_model_name_or_path, *inputs, trust_remote_code=True, **kwargs
+    )
+
 
 class MultiChoice:
     def __init__(self, choices: List[str]) -> None:
@@ -127,6 +139,10 @@ def main() -> None:
 
     assert not args.provide_description  # not implemented
 
+    if "trust_remote_code=True" in args.model_args:
+        AutoTokenizer._from_pretrained = AutoTokenizer.from_pretrained
+        AutoTokenizer.from_pretrained = from_pretrained
+
     if args.limit:
         print(
             "WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT."
diff --git a/models/tokyotech-llm/Swallow-13b-hf/result.json b/models/tokyotech-llm/Swallow-13b-hf/result.json
deleted file mode 100644
index 8d20c81..0000000
--- a/models/tokyotech-llm/Swallow-13b-hf/result.json
+++ /dev/null
@@ -1,80 +0,0 @@
-{
-  "results": {
-    "chabsa-1.0-0.1": {
-      "acc": 0.8643014372653114,
-      "f1": 0.5839097804055466
-    },
-    "cma_basics": {
-      "acc": 0.5263157894736842,
-      "acc_stderr": 0.08208556498579522,
-      "map": 0.7214912280701754,
-      "map_stderr": 0.05224117295038046,
-      "map_2": 0.6447368421052632,
-      "map_2_stderr": 0.07032715345007579,
-      "map_3": 0.6885964912280701,
-      "map_3_stderr": 0.06132301843641161,
-      "map_4": 0.7214912280701754,
-      "map_4_stderr": 0.05224117295038046
-    },
-    "cpa_audit-1.0-0.1.2": {
-      "acc": 0.19597989949748743,
-      "acc_stderr": 0.01992250594300254,
-      "map": 0.43672529313232816,
-      "map_stderr": 0.015248267342186657,
-      "map_2": 0.28391959798994976,
-      "map_2_stderr": 0.020269485126908068,
-      "map_3": 0.34087102177554424,
-      "map_3_stderr": 0.019210444485949288,
-      "map_4": 0.38170016750418756,
-      "map_4_stderr": 0.01790083722714454
-    },
-    "fp2-1.0-0.1.2": {
-      "acc": 0.3263157894736842,
-      "acc_stderr": 0.021535653639171162,
-      "map": 0.5707017543859655,
-      "map_stderr": 0.014236438760583003,
-      "map_2": 0.44526315789473686,
-      "map_2_stderr": 0.019834630794362453,
-      "map_3": 0.5133333333333342,
-      "map_3_stderr": 0.01742468963902119,
-      "map_4": 0.5707017543859655,
-      "map_4_stderr": 0.014236438760583003
-    },
-    "security_sales_1-1.0-0.1.2": {
-      "acc": 0.49122807017543857,
-      "acc_stderr": 0.06680502724442024,
-      "map": 0.7295321637426901,
-      "map_stderr": 0.03467189555109694,
-      "map_2": 0.7192982456140351,
-      "map_2_stderr": 0.037583540738017104,
-      "map_3": 0.7251461988304094,
-      "map_3_stderr": 0.03600682874984946,
-      "map_4": 0.7295321637426901,
-      "map_4_stderr": 0.03467189555109694
-    }
-  },
-  "versions": {
-    "chabsa-1.0-0.1": 1.0,
-    "cma_basics": 1.0,
-    "cpa_audit-1.0-0.1.2": 1.0,
-    "fp2-1.0-0.1.2": 1.0,
-    "security_sales_1-1.0-0.1.2": 1.0
-  },
-  "config": {
-    "model": "hf-causal-experimental",
-    "model_args": "pretrained=tokyotech-llm/Swallow-13b-hf",
-    "num_fewshot": [
-      4,
-      4,
-      1,
-      3,
-      4
-    ],
-    "batch_size": null,
-    "device": null,
-    "no_cache": true,
-    "limit": null,
-    "bootstrap_iters": 100000,
-    "description_dict": {}
-  }
-}
\ No newline at end of file
diff --git a/models/tokyotech-llm/Swallow-13b-instruct-hf/result.json b/models/tokyotech-llm/Swallow-13b-instruct-hf/result.json
deleted file mode 100644
index 077b88e..0000000
--- a/models/tokyotech-llm/Swallow-13b-instruct-hf/result.json
+++ /dev/null
@@ -1,80 +0,0 @@
-{
-  "results": {
-    "chabsa-1.0-0.1": {
-      "acc": 0.8670205878544607,
-      "f1": 0.5852971423322186
-    },
-    "cma_basics": {
-      "acc": 0.6052631578947368,
-      "acc_stderr": 0.0803572542664568,
-      "map": 0.732456140350877,
-      "map_stderr": 0.05000769408329553,
-      "map_2": 0.6578947368421053,
-      "map_2_stderr": 0.06826769112960687,
-      "map_3": 0.719298245614035,
-      "map_3_stderr": 0.05411225269867088,
-      "map_4": 0.732456140350877,
-      "map_4_stderr": 0.05000769408329553
-    },
-    "cpa_audit-1.0-0.1.2": {
-      "acc": 0.19597989949748743,
-      "acc_stderr": 0.019922505943002544,
-      "map": 0.4291457286432159,
-      "map_stderr": 0.014978185526358765,
-      "map_2": 0.28266331658291455,
-      "map_2_stderr": 0.019880657453086006,
-      "map_3": 0.32453936348408685,
-      "map_3_stderr": 0.019140336104312757,
-      "map_4": 0.3710217755443886,
-      "map_4_stderr": 0.017725245100925265
-    },
-    "fp2-1.0-0.1.2": {
-      "acc": 0.35789473684210527,
-      "acc_stderr": 0.0220186962263452,
-      "map": 0.5866666666666672,
-      "map_stderr": 0.014382100663778132,
-      "map_2": 0.4631578947368421,
-      "map_2_stderr": 0.02006045306803082,
-      "map_3": 0.536140350877194,
-      "map_3_stderr": 0.017317107431117744,
-      "map_4": 0.5866666666666672,
-      "map_4_stderr": 0.014382100663778132
-    },
-    "security_sales_1-1.0-0.1.2": {
-      "acc": 0.5789473684210527,
-      "acc_stderr": 0.06597717584505354,
-      "map": 0.7704678362573101,
-      "map_stderr": 0.036624818394750035,
-      "map_2": 0.7368421052631579,
-      "map_2_stderr": 0.045308160876668974,
-      "map_3": 0.7660818713450294,
-      "map_3_stderr": 0.037975530639704706,
-      "map_4": 0.7704678362573101,
-      "map_4_stderr": 0.036624818394750035
-    }
-  },
-  "versions": {
-    "chabsa-1.0-0.1": 1.0,
-    "cma_basics": 1.0,
-    "cpa_audit-1.0-0.1.2": 1.0,
-    "fp2-1.0-0.1.2": 1.0,
-    "security_sales_1-1.0-0.1.2": 1.0
-  },
-  "config": {
-    "model": "hf-causal-experimental",
-    "model_args": "pretrained=tokyotech-llm/Swallow-13b-instruct-hf",
-    "num_fewshot": [
-      4,
-      1,
-      2,
-      3,
-      3
-    ],
-    "batch_size": null,
-    "device": null,
-    "no_cache": true,
-    "limit": null,
-    "bootstrap_iters": 100000,
-    "description_dict": {}
-  }
-}
\ No newline at end of file
diff --git a/models/tokyotech-llm/Swallow-7b-hf/result.json b/models/tokyotech-llm/Swallow-7b-hf/result.json
deleted file mode 100644
index f5667f1..0000000
--- a/models/tokyotech-llm/Swallow-7b-hf/result.json
+++ /dev/null
@@ -1,80 +0,0 @@
-{
-  "results": {
-    "chabsa-1.0-0.5": {
-      "acc": 0.7146186715007121,
-      "f1": 0.4825488937828868
-    },
-    "cma_basics": {
-      "acc": 0.3684210526315789,
-      "acc_stderr": 0.07930218984219833,
-      "map": 0.6557017543859649,
-      "map_stderr": 0.05010766153497393,
-      "map_2": 0.5789473684210527,
-      "map_2_stderr": 0.06674273816118408,
-      "map_3": 0.6228070175438596,
-      "map_3_stderr": 0.058524954700343405,
-      "map_4": 0.6557017543859649,
-      "map_4_stderr": 0.05010766153497393
-    },
-    "cpa_audit-1.0-0.2.1": {
-      "acc": 0.1934673366834171,
-      "acc_stderr": 0.019825290206918044,
-      "map": 0.42931323283082035,
-      "map_stderr": 0.015015812924361145,
-      "map_2": 0.2751256281407035,
-      "map_2_stderr": 0.01998755098400359,
-      "map_3": 0.33123953098827447,
-      "map_3_stderr": 0.018997646003271557,
-      "map_4": 0.37269681742043526,
-      "map_4_stderr": 0.017706146066613367
-    },
-    "fp2-1.0-0.2": {
-      "acc": 0.2968421052631579,
-      "acc_stderr": 0.020984566852914024,
-      "map": 0.5554385964912288,
-      "map_stderr": 0.01410222947523616,
-      "map_2": 0.4189473684210526,
-      "map_2_stderr": 0.019840522700669955,
-      "map_3": 0.49754385964912334,
-      "map_3_stderr": 0.01723223118916302,
-      "map_4": 0.5554385964912288,
-      "map_4_stderr": 0.01410222947523616
-    },
-    "security_sales_1": {
-      "acc": 0.543859649122807,
-      "acc_stderr": 0.06655775300696491,
-      "map": 0.7368421052631579,
-      "map_stderr": 0.038109971923207815,
-      "map_2": 0.7017543859649122,
-      "map_2_stderr": 0.046593966021499254,
-      "map_3": 0.7192982456140351,
-      "map_3_stderr": 0.042781228108049046,
-      "map_4": 0.7368421052631579,
-      "map_4_stderr": 0.038109971923207815
-    }
-  },
-  "versions": {
-    "chabsa-1.0-0.5": 1.0,
-    "cma_basics": 1.0,
-    "cpa_audit-1.0-0.2.1": 1.0,
-    "fp2-1.0-0.2": 1.0,
-    "security_sales_1": 1.0
-  },
-  "config": {
-    "model": "hf-causal-experimental",
-    "model_args": "pretrained=tokyotech-llm/Swallow-7b-hf",
-    "num_fewshot": [
-      0,
-      0,
-      3,
-      4,
-      4
-    ],
-    "batch_size": null,
-    "device": null,
-    "no_cache": true,
-    "limit": null,
-    "bootstrap_iters": 100000,
-    "description_dict": {}
-  }
-}
\ No newline at end of file
diff --git a/models/tokyotech-llm/Swallow-7b-instruct-hf/result.json b/models/tokyotech-llm/Swallow-7b-instruct-hf/result.json
deleted file mode 100644
index 4fb9c4b..0000000
--- a/models/tokyotech-llm/Swallow-7b-instruct-hf/result.json
+++ /dev/null
@@ -1,80 +0,0 @@
-{
-  "results": {
-    "chabsa-1.0-0.6": {
-      "acc": 0.8290819629677586,
-      "f1": 0.5576084597674228
-    },
-    "cma_basics-1.0-0.1": {
-      "acc": 0.3157894736842105,
-      "acc_stderr": 0.07641750223933595,
-      "map": 0.5438596491228069,
-      "map_stderr": 0.05266120550696056,
-      "map_2": 0.39473684210526316,
-      "map_2_stderr": 0.07341956131188584,
-      "map_3": 0.46491228070175444,
-      "map_3_stderr": 0.06627370397214283,
-      "map_4": 0.5438596491228069,
-      "map_4_stderr": 0.05266120550696056
-    },
-    "cpa_audit-1.0-0.2.1": {
-      "acc": 0.19095477386934673,
-      "acc_stderr": 0.019726789326781242,
-      "map": 0.42211055276381865,
-      "map_stderr": 0.015001328869131353,
-      "map_2": 0.271356783919598,
-      "map_2_stderr": 0.019839722596750056,
-      "map_3": 0.31993299832495786,
-      "map_3_stderr": 0.01902765093482286,
-      "map_4": 0.35762144053601336,
-      "map_4_stderr": 0.01792937339034131
-    },
-    "fp2-1.0-0.3": {
-      "acc": 0.2463157894736842,
-      "acc_stderr": 0.01979026746533386,
-      "map": 0.518596491228071,
-      "map_stderr": 0.013229415511298371,
-      "map_2": 0.37157894736842106,
-      "map_2_stderr": 0.018928235707350024,
-      "map_3": 0.4585964912280708,
-      "map_3_stderr": 0.016342190164885553,
-      "map_4": 0.518596491228071,
-      "map_4_stderr": 0.013229415511298371
-    },
-    "security_sales_1-1.0-0.1.2": {
-      "acc": 0.49122807017543857,
-      "acc_stderr": 0.06680502724442024,
-      "map": 0.7090643274853802,
-      "map_stderr": 0.0379000754952048,
-      "map_2": 0.6666666666666666,
-      "map_2_stderr": 0.04738350265803164,
-      "map_3": 0.6959064327485381,
-      "map_3_stderr": 0.0413254211260309,
-      "map_4": 0.7090643274853802,
-      "map_4_stderr": 0.0379000754952048
-    }
-  },
-  "versions": {
-    "chabsa-1.0-0.6": 1.0,
-    "cma_basics-1.0-0.1": 1.0,
-    "cpa_audit-1.0-0.2.1": 1.0,
-    "fp2-1.0-0.3": 1.0,
-    "security_sales_1-1.0-0.1.2": 1.0
-  },
-  "config": {
-    "model": "hf-causal-experimental",
-    "model_args": "pretrained=tokyotech-llm/Swallow-7b-instruct-hf",
-    "num_fewshot": [
-      1,
-      3,
-      3,
-      4,
-      0
-    ],
-    "batch_size": null,
-    "device": null,
-    "no_cache": true,
-    "limit": null,
-    "bootstrap_iters": 100000,
-    "description_dict": {}
-  }
-}
\ No newline at end of file
diff --git a/pyproject.toml b/pyproject.toml
index 03743db..fcd7603 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -22,6 +22,9 @@ protobuf = "^4.24.4"
 openpyxl = "^3.1.2"
 xlrd = "^2.0.1"
 openai = "0.28.1"
+transformers = "^4.36.2"
+tiktoken = "^0.5.2"
+transformers-stream-generator = "^0.0.4"
 
 
 [tool.poetry.group.dev.dependencies]
